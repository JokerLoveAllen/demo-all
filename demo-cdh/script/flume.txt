# Please paste flume.conf here. Example:
# Sources, channels, and sinks are defined per
# agent name, in this case 'tier1'.
##  tier1.sources  = source1
##  tier1.channels = channel1
##  tier1.sinks    = sink1
# For each source, channel, and sink, set
# standard properties.
## tier1.sources.source1.type     = netcat
## tier1.sources.source1.bind     = 127.0.0.1
## tier1.sources.source1.port     = 9999
## tier1.sources.source1.channels = channel1
## tier1.channels.channel1.type   = memory
## tier1.sinks.sink1.type         = logger
## tier1.sinks.sink1.channel      = channel1
# Other properties are specific to each type of
# source, channel, or sink. In this case, we
# specify the capacity of the memory channel.
## tier1.channels.channel1.capacity = 100



# flume dirs to hdfs example:
# Name the components on this agent
## tier1.sources = r1
## tier1.sinks = k1
## tier1.channels = c1
# Describe/configure the source
##注意：不能往监控目中重复丢同名文件
## tier1.sources.r1.type = spooldir
## tier1.sources.r1.spoolDir = /data/flume/testLogFile
## tier1.sources.r1.fileHeader = true
# Describe the sink
## tier1.sinks.k1.type = hdfs
## tier1.sinks.k1.channel = c1
## tier1.sinks.k1.hdfs.path = hdfs://hadoop01:8020/flume/spooldir/files/%y-%m-%d/%H%M/
## tier1.sinks.k1.hdfs.filePrefix = events-
## tier1.sinks.k1.hdfs.round = true
## tier1.sinks.k1.hdfs.roundValue = 10
## tier1.sinks.k1.hdfs.roundUnit = minute
## tier1.sinks.k1.hdfs.rollInterval = 3

## tier1.sinks.k1.hdfs.rollSize = 20
## tier1.sinks.k1.hdfs.rollCount = 5
## tier1.sinks.k1.hdfs.batchSize = 1
## tier1.sinks.k1.hdfs.useLocalTimeStamp = true

#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本
## tier1.sinks.k1.hdfs.fileType = DataStream

# Use a channel which buffers events in memory
## tier1.channels.c1.type = memory
## tier1.channels.c1.capacity = 1000
## tier1.channels.c1.transactionCapacity = 100
# Bind the source and sink to the channel
## tier1.sources.r1.channels = c1
## tier1.sinks.k1.channel = c1



# flume appendable file  to hdfs example:
tier1.sources = source1
tier1.sinks = sink1
tier1.channels = channel1
# Describe/configure tail -F source1
tier1.sources.source1.type = exec
tier1.sources.source1.command = tail -F  /data/flume/testLogFile/test.log
tier1.sources.source1.channels = channel1
# Describe sink1
tier1.sinks.sink1.type = hdfs
#a1.sinks.k1.channel = c1
tier1.sinks.sink1.hdfs.path = hdfs://hadoop01:8020/flume/flume-collection/%y-%m-%d/%H-%M
tier1.sinks.sink1.hdfs.filePrefix = test
tier1.sinks.sink1.hdfs.maxOpenFiles = 5000
tier1.sinks.sink1.hdfs.batchSize= 100
tier1.sinks.sink1.hdfs.fileType = DataStream
tier1.sinks.sink1.hdfs.writeFormat =Text
tier1.sinks.sink1.hdfs.round = true
tier1.sinks.sink1.hdfs.roundValue = 10
tier1.sinks.sink1.hdfs.roundUnit = minute
tier1.sinks.sink1.hdfs.useLocalTimeStamp = true
# Use a channel which buffers events in memory
tier1.channels.channel1.type = memory
tier1.channels.channel1.keep-alive = 120
tier1.channels.channel1.capacity = 500000
tier1.channels.channel1.transactionCapacity = 600
# Bind the source and sink to the channel
tier1.sources.source1.channels = channel1
tier1.sinks.sink1.channel = channel1